{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script opens ICESat-02 ATL08 v6 data to export photons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, file structure should be as follows:\n",
    "- The current working directory contains this script, and separate subdirectories for raw data and for extracted data\n",
    "- This script will be the middleman, pulling data and writing data between these two subdirectories\n",
    "\n",
    "                                                 Current Working Directory\n",
    "___________________________________________________________________________________________________________________________\n",
    "                 ↓                                     ↓                                            ↓\n",
    "            ./rawData/    <------->     height_extraction.ipynb (this script)    <------->  ./extractedData/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting our current working directory to proper location, and subdirectories for raw & extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(r'D:/Icesat')\n",
    "os.getcwd()\n",
    "os.listdir()\n",
    "rawData = './rawData'\n",
    "rawDataTesting = './rawDataTesting'\n",
    "rawDataTestingLarger = './rawDataTestingLarger'\n",
    "extractedData = './extractedData'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary packages: h5py, numpy, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "import shapely as spl\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr. Thomas's instructions: \"Extract relevant data from h5 file (h_canopy, h_canopy_uncertainty, night_flag, lat, long, and file name (because the beam and date is contained in the name) into text or whatever (we already have some code to do this).   You will want a file with the following information from icesat: Shotnumber, date, beam (or ground track), lat, lon, h_canopy and associated metrics, 20m segment heights, night_flag, h_canopy_uncertainty.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High level overview of code: \n",
    "For every file in our 5000 file dataset, and for every beam of the 6 beams within that specific file, we'll assign variables to parameters of interest (lat, long, night_flag) and write those values to csv \n",
    "\n",
    "Outer loop: current file in subdirectory\n",
    "Inner loop: beams of current file\n",
    "Innermost code: assigning, reshaping, and getting all parameters into a single table that we'll write to csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of column names (1, 35)\n"
     ]
    }
   ],
   "source": [
    "# First creating a row for column names\n",
    "# Doing this before any files are read as we only want 1 row of column names,\n",
    "# rather than repeating column names every time a new file is active\n",
    "column_names = np.array((['file name',\n",
    "                'date',\n",
    "                'beam name',\n",
    "                'latitude',\n",
    "                'longitude',\n",
    "                'spacecraft orientation',\n",
    "                'canopy height',\n",
    "                'canopy height uncertainty',\n",
    "                'canopy height metric 1',\n",
    "                'canopy height metric 2', \n",
    "                'canopy height metric 3', \n",
    "                'canopy height metric 4', \n",
    "                'canopy height metric 5', \n",
    "                'canopy height metric 6', \n",
    "                'canopy height metric 7', \n",
    "                'canopy height metric 8', \n",
    "                'canopy height metric 9', \n",
    "                'canopy height metric 10', \n",
    "                'canopy height metric 11', \n",
    "                'canopy height metric 12', \n",
    "                'canopy height metric 13', \n",
    "                'canopy height metric 14', \n",
    "                'canopy height metric 15', \n",
    "                'canopy height metric 16', \n",
    "                'canopy height metric 17',\n",
    "                'canopy height metric 18', \n",
    "                'canopy height 0-20m',\n",
    "                'canopy height 20-40m',\n",
    "                'canopy height 40-60m',\n",
    "                'canopy height 60-80m',\n",
    "                'canopy height 80-100m',\n",
    "                'canopy openness',\n",
    "                'segment id beginning',\n",
    "                'segment id ending',\n",
    "                'night flag']))\n",
    "\n",
    "num = 0\n",
    "for name in column_names:\n",
    "    num = num + 1\n",
    "column_names = pd.DataFrame(column_names.reshape((1, 35)))\n",
    "print('Dimensions of column names',column_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBeams(file):\n",
    "    # Getting all the beams for a file\n",
    "    allKeys = list(file.keys())\n",
    "    \n",
    "    # number of keys that begin with 'gt', or in otherwords a beam\n",
    "    beamCount = sum('gt' in key for key in list(file.keys()))\n",
    "    \n",
    "    # surfType is the parameter just before the keys begin\n",
    "    surfTypeIndex = allKeys.index('ds_surf_type')\n",
    "    \n",
    "    # slicing the allKeys array from surfType to the index of surfType + number of beams\n",
    "    beams = np.array(allKeys[surfTypeIndex + 1 : surfTypeIndex + beamCount + 1])\n",
    "    \n",
    "    # print('file:', file, 'has the following {} beams'.format(beams.shape[0]), beams)\n",
    "    \n",
    "    # Return an array for all the beams only in that specific file (could be 6, could be only 2)\n",
    "    return beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Workhorse function to extract data from a given file\n",
    "def extractFileData(file, beams, outputFile):\n",
    "    \n",
    "    rowsInFile = 0\n",
    "\n",
    "    # Assigning an array to the beams present in the specific file\n",
    "    print('file beams:', beams)\n",
    "    \n",
    "    # Column for spacecraft orientation\n",
    "    orientation = file['orbit_info/sc_orient'][0]\n",
    "        \n",
    "    # Looping through all the beams for the file\n",
    "    # Keep in mind that any given beam may or may not have the land segments group\n",
    "    for beam in beams:\n",
    "        \n",
    "        # If there is a land_segments flag, then we want to extract the data and put it into a text file\n",
    "        if '{}/land_segments'.format(beam) in file:\n",
    "            print('{} does have a land_segments flag'.format(beam))\n",
    "            rowsInBeam = writeBeamToFile(file, beam, outputFile)\n",
    "            \n",
    "            rowsInFile = rowsInFile + rowsInBeam\n",
    "        else:\n",
    "            print('{} DOES NOT have a land_segments flag'.format(beam))\n",
    "            \n",
    "        # return df\n",
    "    print('file', file, 'has', rowsInFile, 'total rows')\n",
    "    return rowsInFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to assign H5 dataframes to arrays and write to a file\n",
    "def writeBeamToFile(file, beam, outputFile):\n",
    "    latitude = file[beam + '/land_segments/latitude']\n",
    "    latitude = np.array((latitude)).reshape((latitude.shape[0], 1))\n",
    "    # print('beam', beam, 'has', latitude.shape[0], 'records in the latitude flag')\n",
    "\n",
    "\n",
    "    longitude = file[beam + '/land_segments/longitude']\n",
    "    longitude = np.array((longitude)).reshape((longitude.shape[0], 1))\n",
    "    # print('beam', beam, 'has', latitude.shape[0], 'records in the longitude flag')\n",
    "   \n",
    "    # Column for spacecraft orientation, reassigned in loop to match length of latitude records for particular beam \n",
    "    orientation = file['orbit_info/sc_orient'][0]\n",
    "    sc_orient = np.full_like(longitude, orientation, dtype=int)\n",
    "    # print(sc_orient[0:2])\n",
    "\n",
    "    # Column for canopy height             \n",
    "    h_canopy = file[beam + '/land_segments/canopy/h_canopy']\n",
    "    h_canopy = np.array(h_canopy).reshape((h_canopy.shape[0], 1))       \n",
    "\n",
    "    # Column for canopy height uncertainty\n",
    "    h_canopy_uncertainty = file[beam + '/land_segments/canopy/h_canopy_uncertainty']\n",
    "    h_canopy_uncertainty = np.array(h_canopy_uncertainty).reshape((h_canopy_uncertainty.shape[0], 1))        \n",
    "\n",
    "    # This metric is NOT reshaped into a 1 column array, it has dimensions of ( number of transects x 18)\n",
    "    canopy_h_metrics = file[beam + '/land_segments/canopy/canopy_h_metrics']\n",
    "    canopy_h_metrics = np.array(canopy_h_metrics)\n",
    "\n",
    "    # This metric is also NOT reshaped into a 1 column array, it has dimensions of ( number of transects x 5)\n",
    "    h_canopy_20m = file[beam + '/land_segments/canopy/h_canopy_20m/']\n",
    "    h_canopy_20m = np.array(h_canopy_20m)\n",
    "\n",
    "    # Column for the canopy openness\n",
    "    canopy_openness = file[beam + '/land_segments/canopy/canopy_openness']\n",
    "    canopy_openness = np.array(canopy_openness).reshape((canopy_openness.shape[0], 1))\n",
    "\n",
    "    # Column for the segment ID beginning\n",
    "    segment_id_beg = file[beam + '/land_segments/segment_id_beg']\n",
    "    segment_id_beg = np.array(segment_id_beg).reshape((segment_id_beg.shape[0], 1))\n",
    "    # print('Segment ID beginning', segment_id_beg[0:2])\n",
    "\n",
    "    # Column for the segment ID ending\n",
    "    segment_id_end = file[beam + '/land_segments/segment_id_end']\n",
    "    segment_id_end = np.array(segment_id_end).reshape((segment_id_end.shape[0], 1))\n",
    "\n",
    "    # Column for the night flag\n",
    "    night_flag = file[beam + '/land_segments/night_flag']\n",
    "    night_flag = np.array(night_flag).reshape((night_flag.shape[0], 1))\n",
    "\n",
    "    # Column for the specific file name, same dimensions as latitude column\n",
    "    # Though this is the first column in the eventual csv, it is lower in this loop as we dont yet know # of records per specific beam. \n",
    "    # That information is obtained with the latitude flag at the start of the loop\n",
    "    file_name = np.full_like(latitude, filename[:-3], dtype=np.dtype('U100'))\n",
    "\n",
    "    # Creating a column for the granule date.\n",
    "    # HMS data can be parsed later as there's inconsistencies between file names and the granule START time listed in the Earthdata portal. \n",
    "    # Creating a datetime object of the filename string, then a column of just the datetime information\n",
    "    date_time = datetime.strptime(filename[filename.index('_') + 1:filename.index('_') + 15], \"%Y%m%d%H%M%S\")\n",
    "    date = np.full_like(latitude, date_time, dtype=np.dtype('U100'))\n",
    "    date = np.array(date).reshape((date.shape[0], 1))\n",
    "\n",
    "    # Creating a column for the specific beam name, same dimensions as the latitude column\n",
    "    beam_name = np.full_like(file_name, beam, dtype=np.dtype('U100'))            \n",
    "    \n",
    "    rowsInBeam = 0\n",
    "    rowsInBeam = latitude.shape[0]\n",
    "    print('beam',beam,'has', rowsInBeam, 'records')\n",
    "\n",
    "    # overall_records = overall_records + records_in_beam\n",
    "    # print('output csv will have', overall_records, 'records with addition of', beam)\n",
    "\n",
    "    df = pd.DataFrame(np.hstack((\n",
    "            file_name,\n",
    "            date,\n",
    "            beam_name,            \n",
    "            latitude, \n",
    "            longitude, \n",
    "            sc_orient, \n",
    "            h_canopy, \n",
    "            h_canopy_uncertainty, \n",
    "            canopy_h_metrics, \n",
    "            h_canopy_20m,\n",
    "            canopy_openness, \n",
    "            segment_id_beg, \n",
    "            segment_id_end, \n",
    "            night_flag                            \n",
    "        )))\n",
    "    df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    return rowsInBeam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def writeBeamToFile(df, output_file):\n",
    "#     df.to_csv(output_file, mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code block below is smaller version of main chunk, used just for looping through rawDataTesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broad level structure:\n",
    "- For every file in subdirectory:\n",
    "    - create file object for current file\n",
    "    - get beam information for current file:\n",
    "    - for every beam in current file:\n",
    "        - create a dataframe of the parameters\n",
    "        - write the dataframe to a textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20190323011928_12920206_006_02.h5\n",
      "File open FAILURE\n",
      "Unable to open file (file signature not found)\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20190328005358_13680207_006_02.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 50 records\n",
      "gt1r DOES NOT have a land_segments flag\n",
      "file <HDF5 file \"ATL08_20190328005358_13680207_006_02.h5\" (mode r)> has 50 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20190715084425_02640402_006_02.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 132 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 93 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 97 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 68 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 75 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 44 records\n",
      "file <HDF5 file \"ATL08_20190715084425_02640402_006_02.h5\" (mode r)> has 509 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20200629023019_00570806_006_01.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 168 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 9 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 200 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 58 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 295 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 131 records\n",
      "file <HDF5 file \"ATL08_20200629023019_00570806_006_01.h5\" (mode r)> has 861 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20200913233455_12310807_006_02.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 314 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 165 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 30 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 5 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 175 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 4 records\n",
      "file <HDF5 file \"ATL08_20200913233455_12310807_006_02.h5\" (mode r)> has 693 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20210402025837_01271101_006_02.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt2l' 'gt2r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 3 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 2 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 1 records\n",
      "file <HDF5 file \"ATL08_20210402025837_01271101_006_02.h5\" (mode r)> has 6 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20210521122436_08811106_006_01.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 65 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 147 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 54 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 115 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 52 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 140 records\n",
      "file <HDF5 file \"ATL08_20210521122436_08811106_006_01.h5\" (mode r)> has 573 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20210725081125_04841206_006_01.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 38 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 80 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 93 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 175 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 16 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 52 records\n",
      "file <HDF5 file \"ATL08_20210725081125_04841206_006_01.h5\" (mode r)> has 454 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20210921173327_13761202_006_02.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 7 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 13 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 14 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 80 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 15 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 59 records\n",
      "file <HDF5 file \"ATL08_20210921173327_13761202_006_02.h5\" (mode r)> has 188 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20211127015330_10021306_006_01.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 287 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 25 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 238 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 11 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 263 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 37 records\n",
      "file <HDF5 file \"ATL08_20211127015330_10021306_006_01.h5\" (mode r)> has 861 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20220421083052_04471501_006_02.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l DOES NOT have a land_segments flag\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 7 records\n",
      "gt2r DOES NOT have a land_segments flag\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 57 records\n",
      "gt3r DOES NOT have a land_segments flag\n",
      "file <HDF5 file \"ATL08_20220421083052_04471501_006_02.h5\" (mode r)> has 64 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20220501072956_05991502_006_01.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 177 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 56 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 136 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 74 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 151 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 65 records\n",
      "file <HDF5 file \"ATL08_20220501072956_05991502_006_01.h5\" (mode r)> has 659 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20221222201000_00351802_006_01.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 106 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 120 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 50 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 64 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 4 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 3 records\n",
      "file <HDF5 file \"ATL08_20221222201000_00351802_006_01.h5\" (mode r)> has 347 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20230117182842_04311802_006_01.h5\n",
      "File open success!\n",
      "file beams: ['gt3l' 'gt3r']\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 93 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 93 records\n",
      "file <HDF5 file \"ATL08_20230117182842_04311802_006_01.h5\" (mode r)> has 186 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20230317043947_13231806_006_01.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 96 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 42 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 111 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 90 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 176 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 98 records\n",
      "file <HDF5 file \"ATL08_20230317043947_13231806_006_01.h5\" (mode r)> has 613 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20230620104617_00042002_006_01.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 14902 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 10505 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 13308 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 9751 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 13808 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 9649 records\n",
      "file <HDF5 file \"ATL08_20230620104617_00042002_006_01.h5\" (mode r)> has 71923 total rows\n",
      "\n",
      "current file: ./rawDataTestingLarger\\ATL08_20230621231925_00272006_006_01.h5\n",
      "File open success!\n",
      "file beams: ['gt1l' 'gt1r' 'gt2l' 'gt2r' 'gt3l' 'gt3r']\n",
      "gt1l does have a land_segments flag\n",
      "beam gt1l has 13461 records\n",
      "gt1r does have a land_segments flag\n",
      "beam gt1r has 8592 records\n",
      "gt2l does have a land_segments flag\n",
      "beam gt2l has 11990 records\n",
      "gt2r does have a land_segments flag\n",
      "beam gt2r has 8136 records\n",
      "gt3l does have a land_segments flag\n",
      "beam gt3l has 13548 records\n",
      "gt3r does have a land_segments flag\n",
      "beam gt3r has 8399 records\n",
      "file <HDF5 file \"ATL08_20230621231925_00272006_006_01.h5\" (mode r)> has 64126 total rows\n",
      "\n",
      "total files extracted: 17\n",
      "\n",
      "total rows in output txt file 142113\n"
     ]
    }
   ],
   "source": [
    "# Creating an output file and clearing it's contents\n",
    "outputFile = \"./extractedData/output.txt\"\n",
    "open(outputFile, 'w').close()\n",
    "\n",
    "filesExtracted = 0\n",
    "totalRows = 0\n",
    "rowsInFile = 0\n",
    "\n",
    "# Looping through all files in directory\n",
    "for filename in sorted(os.listdir(rawDataTestingLarger)):\n",
    "    file = os.path.join(rawDataTestingLarger, filename)\n",
    "    print('\\ncurrent file:', file)\n",
    "   \n",
    "    # Testing that the current file path is valid by opening it with h5py object\n",
    "    try:\n",
    "        file = h5py.File(file, 'r')\n",
    "        print('File open success!')\n",
    "        \n",
    "        # If successfully opened, extract the data from file (main code chunk) and write to .txt \n",
    "        beams = getBeams(file)\n",
    "        rowsInFile = extractFileData(file, beams, outputFile)\n",
    "        # writeBeamToFile(df, output_file)\n",
    "        \n",
    "    # If file can't be opened, printing error message and moving on\n",
    "    except IOError as e:\n",
    "        print('File open FAILURE')\n",
    "        print(str(e))\n",
    "    \n",
    "    filesExtracted = filesExtracted + 1\n",
    "    totalRows = totalRows + rowsInFile\n",
    "\n",
    "print('\\ntotal files extracted:', filesExtracted)\n",
    "print('\\ntotal rows in output txt file', totalRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the old code chunk for looping through the files, leave it untouched and use it to copy chunks of code for more elegant looping with function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file: ATL08_20190323011928_12920206_006_02.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# checking if it is a file, assigning an array to all the file's keys\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file):\n\u001b[1;32m---> 22\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     keys \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(file\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[0;32m     24\u001b[0m     beams \u001b[38;5;241m=\u001b[39m getBeams(file)        \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    558\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    560\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    561\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    562\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    563\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    564\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    565\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    566\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 567\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    230\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 231\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    233\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "# Creating an output file and clearing it's contents\n",
    "output_file = \"./extractedData/output.txt\"\n",
    "open(output_file, 'w').close()\n",
    "\n",
    "# Test variable to track number of measurements in current beam\n",
    "# Test variable for running tally of overall records \n",
    "# records_in_beam = 0\n",
    "# overall_records = 0\n",
    "\n",
    "# Adding our previous column names array to the csv for easier Excel import\n",
    "column_names.to_csv(\"./extractedData/output.txt\", mode='a', index=False, header=False)\n",
    "\n",
    "# totalFiles = 0\n",
    "\n",
    "for filename in sorted(os.listdir(rawDataTesting)):\n",
    "    file = os.path.join(rawDataTesting, filename)\n",
    "    print('Current file:', filename)\n",
    "    \n",
    "    \n",
    "    # checking if it is a file, assigning an array to all the file's keys\n",
    "    if os.path.isfile(file):\n",
    "        file = h5py.File(file, 'r')\n",
    "        keys = np.array(list(file.keys()))\n",
    "        beams = getBeams(file)        \n",
    "        print('Number of beams:', beams.shape[0])\n",
    "\n",
    "    # Column for spacecraft orientation\n",
    "    orientation = file['orbit_info/sc_orient'][0]\n",
    "        \n",
    "    # Looping through all the beams for the granule\n",
    "    # Keep in mind that any given beam may or may not have the land segments group\n",
    "    for beam in beams:\n",
    "        \n",
    "            if '{}/land_segments'.format(beam) in file:\n",
    "\n",
    "                latitude = file[beam + '/land_segments/latitude']\n",
    "                latitude = np.array((latitude)).reshape((latitude.shape[0], 1))\n",
    "\n",
    "                longitude = file[beam + '/land_segments/longitude']\n",
    "                longitude = np.array((longitude)).reshape((longitude.shape[0], 1))\n",
    "\n",
    "                # creating a matrix for the vertices of our rectangle\n",
    "                # rectangleVertices = drawPolygons(longitude, latitude)            \n",
    "\n",
    "                # Column for spacecraft orientation, reassigned in loop to match length of latitude records for particular beam\n",
    "                sc_orient = np.full_like(longitude, orientation, dtype=int)\n",
    "                # print(sc_orient[0:2])\n",
    "\n",
    "                # Column for canopy height             \n",
    "                h_canopy = file[beam + '/land_segments/canopy/h_canopy']\n",
    "                h_canopy = np.array(h_canopy).reshape((h_canopy.shape[0], 1))       \n",
    "\n",
    "                # Column for canopy height uncertainty\n",
    "                h_canopy_uncertainty = file[beam + '/land_segments/canopy/h_canopy_uncertainty']\n",
    "                h_canopy_uncertainty = np.array(h_canopy_uncertainty).reshape((h_canopy_uncertainty.shape[0], 1))        \n",
    "\n",
    "                # This metric is NOT reshaped into a 1 column array, it has dimensions of ( number of transects x 18)\n",
    "                canopy_h_metrics = file[beam + '/land_segments/canopy/canopy_h_metrics']\n",
    "                canopy_h_metrics = np.array(canopy_h_metrics)\n",
    "\n",
    "                # This metric is also NOT reshaped into a 1 column array, it has dimensions of ( number of transects x 5)\n",
    "                h_canopy_20m = file[beam + '/land_segments/canopy/h_canopy_20m/']\n",
    "                h_canopy_20m = np.array(h_canopy_20m)\n",
    "\n",
    "                # Column for the canopy openness\n",
    "                canopy_openness = file[beam + '/land_segments/canopy/canopy_openness']\n",
    "                canopy_openness = np.array(canopy_openness).reshape((canopy_openness.shape[0], 1))\n",
    "\n",
    "                # Column for the segment ID beginning\n",
    "                segment_id_beg = file[beam + '/land_segments/segment_id_beg']\n",
    "                segment_id_beg = np.array(segment_id_beg).reshape((segment_id_beg.shape[0], 1))\n",
    "                # print('Segment ID beginning', segment_id_beg[0:2])\n",
    "\n",
    "                # Column for the segment ID ending\n",
    "                segment_id_end = file[beam + '/land_segments/segment_id_end']\n",
    "                segment_id_end = np.array(segment_id_end).reshape((segment_id_end.shape[0], 1))\n",
    "\n",
    "                # Column for the night flag\n",
    "                night_flag = file[beam + '/land_segments/night_flag']\n",
    "                night_flag = np.array(night_flag).reshape((night_flag.shape[0], 1))\n",
    "\n",
    "                # Column for the specific file name, same dimensions as latitude column\n",
    "                # Though this is the first column in the eventual csv, it is lower in this loop as we dont yet know # of records per specific beam. \n",
    "                # That information is obtained with the latitude flag at the start of the loop\n",
    "                file_name = np.full_like(latitude, filename[:-3], dtype=np.dtype('U100'))\n",
    "\n",
    "                # Creating a column for the granule date.\n",
    "                # HMS data can be parsed later as there's inconsistencies between file names and the granule START time listed in the Earthdata portal. \n",
    "                # Creating a datetime object of the filename string, then a column of just the datetime information\n",
    "                date_time = datetime.strptime(filename[filename.index('_') + 1:filename.index('_') + 15], \"%Y%m%d%H%M%S\")\n",
    "                date = np.full_like(latitude, date_time, dtype=np.dtype('U100'))\n",
    "                date = np.array(date).reshape((date.shape[0], 1))\n",
    "\n",
    "                # Creating a column for the specific beam name, same dimensions as the latitude column\n",
    "                beam_name = np.full_like(file_name, beam, dtype=np.dtype('U100'))            \n",
    "\n",
    "                records_in_beam = latitude.shape[0]\n",
    "                # print('beam',beam,'has', records_in_beam, 'records')\n",
    "\n",
    "                overall_records = overall_records + records_in_beam\n",
    "                # print('output csv will have', overall_records, 'records with addition of', beam)\n",
    "\n",
    "                df = pd.DataFrame(np.hstack((\n",
    "                        file_name,\n",
    "                        date,\n",
    "                        beam_name,            \n",
    "                        latitude, \n",
    "                        longitude, \n",
    "                        sc_orient, \n",
    "                        h_canopy, \n",
    "                        h_canopy_uncertainty, \n",
    "                        canopy_h_metrics, \n",
    "                        h_canopy_20m,\n",
    "                        canopy_openness, \n",
    "                        segment_id_beg, \n",
    "                        segment_id_end, \n",
    "                        night_flag                            \n",
    "                    )))\n",
    "\n",
    "                df.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    print('Success! File done.')\n",
    "    totalFiles = totalFiles + 1\n",
    "            \n",
    "            \n",
    "print('Total files read:', totalFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def drawPolygons(latitude, longitude):\n",
    "#     # input: array of points in their latitude & longitude form\n",
    "#     # output: array of rectangles\n",
    "#     # LONGITUDE = X, LATITUDE = y\n",
    "    \n",
    "#     rectangleVertices = np.zeros((latitude.shape[0], 4), dtype=np.double)\n",
    "#     print(rectangleVertices.shape)\n",
    "    \n",
    "#     coordinates = np.hstack((longitude, latitude))\n",
    "#     for coordinate in coordinates:\n",
    "#         long = coordinate[0]\n",
    "#         lat = coordinate[1]\n",
    "        \n",
    "        # INSERT CODE HERE TO CONVERT COORDINATES TO LOCAL UTM\n",
    "        # longMeters = \n",
    "        # latMeters = \n",
    "    \n",
    "        \n",
    "        # topLeft = (longMeters - 4.75903, lat + 49.9695)\n",
    "        # topRight = (longMeters + 8.24897, lat + 49.9695)\n",
    "        # bottomRight = (longMeters + 4.75903, lat - 49.9695)\n",
    "        # bottomLeft = (longMeters - 8.24897, lat - 49.9695)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code crashed on file: ATL08_20230117182842_04311802_006_01.h5 \n",
    "- Previous files had all 6 keys, this one only had 2\n",
    "- Need to accommodate varying number of keys rather than assume they all have 6\n",
    "- Lets try the next file: ATL08_20230118072730_04391806_006_01.h5\n",
    "- Next one has 6\n",
    "- Code also crashed on ATL08_20190328005358_13680207_006_02.h5, as there's only 2 beams\n",
    "- for one of the beams there is no land_segments key\n",
    "- need to incorporate error handling for if there's no land segments key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This beam does have a land_segments group\n"
     ]
    }
   ],
   "source": [
    "# This file only has 2 beams\n",
    "# Of the two beams, gt1r has no land_segments group\n",
    "\n",
    "# errorFile = 'rawDataTesting/ATL08_20190328005358_13680207_006_02.h5'\n",
    "# errorFile = h5py.File(errorFile, 'r')\n",
    "# list(errorFile['gt1r'].keys())\n",
    "# if 'gt1l/land_segments' in errorFile:\n",
    "#     print('This beam does have a land_segments group')\n",
    "# else:\n",
    "#     print(' This beam DOES NOT have a land_segments group')\n",
    "# 0getBeams(errorFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code loaded about 33,000,000 lines which I loaded into an excel query \n",
    "Ran into an issue with one of the files\n",
    "Divide file directory into 20 subfiles\n",
    "Go 250 files, then move onto another .txt file\n",
    "Use JMP to view files\n",
    "Text editor for dealing with gigantic files\n",
    "Column edits in a text editor\n",
    "ultraEdit or sublimeText\n",
    "Linux Commands for printing first few lines of text file\n",
    "Texas A&M Moambo, Propescu, Amy Neuenschwander\n",
    "GEDI Dubai, Armsten, Duncanson\n",
    "Not all monolithic, lean toward RH98\n",
    "Look into recent releases using V005 or V006 data\n",
    "Try initially to get a reduced geographic area\n",
    "1 dataset of what geographic area is, broken into smaller files\n",
    "\n",
    "Spring Courses:\n",
    "- Val's lidar\n",
    "- Quinn Thomas's course?\n",
    "- Potentially Machine learning with Anuj\n",
    "- Talk to Anuj about machine learning and linear algebra background\n",
    "- either biometry or stats & research (5616)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Error handling for missing keys\n",
    "# Relevant stackOverflow problems\n",
    "# https://stackoverflow.com/questions/7675838/python-hdf5-h5py-issues-opening-multiple-files?rq=3\n",
    "# https://stackoverflow.com/questions/55473368/h5py-randomly-unable-to-open-object-component-not-found\n",
    "# https://stackoverflow.com/questions/41474634/python-unable-to-open-a-h5-file\n",
    "# Think there's some funny business with the back slashes in the directory assigments, double check about escapes\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
