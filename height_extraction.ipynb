{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script opens ICESat-02 ATL08 v6 data to export photons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, file structure should be as follows:\n",
    "- The current working directory contains this script, and separate subdirectories for raw data and for extracted data\n",
    "- This script will be pulling data and writing data between these two subdirectories\n",
    "\n",
    "                                                 Current Working Directory\n",
    "___________________________________________________________________________________________________________________________\n",
    "                 ↓                                     ↓                                            ↓\n",
    "            ./Raw Data/    <------->     height_extraction.ipynb (this script)    <------->  ./Extracted Data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting our current working directory to proper location, and subdirectories for raw & extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('C:\\\\Users\\\\17865\\\\OneDrive\\\\Desktop\\\\Masters\\\\Research\\\\IceSat\\\\Notebook\\\\')\n",
    "os.getcwd()\n",
    "raw_data = 'C:\\\\Users\\\\17865\\\\OneDrive\\\\Desktop\\\\Masters\\\\Research\\\\IceSat\\\\Notebook\\\\Raw Data'\n",
    "extracted_data = 'C:\\\\Users\\\\17865\\\\OneDrive\\\\Desktop\\\\Masters\\\\Research\\\\IceSat\\\\Notebook\\\\Extracted Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary packages: h5py, numpy, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning an object to our test H5 file found within the Raw Data subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_file = \"./Raw Data/ATL08_20230621231925_00272006_006_01.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['METADATA',\n",
       " 'ancillary_data',\n",
       " 'ds_geosegments',\n",
       " 'ds_metrics',\n",
       " 'ds_surf_type',\n",
       " 'gt1l',\n",
       " 'gt1r',\n",
       " 'gt2l',\n",
       " 'gt2r',\n",
       " 'gt3l',\n",
       " 'gt3r',\n",
       " 'orbit_info',\n",
       " 'quality_assessment']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File(input_file, 'r')\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The file has numerous keys including one for each beam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an array for all the beams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt1r\n"
     ]
    }
   ],
   "source": [
    "beams = np.array(('gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r'))\n",
    "print(beams[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each beam is also its own group, which has keys within it. The 'land segments' group contains our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['signal_photons', 'land_segments']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt1r = f['gt1r']\n",
    "list(gt1r.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr. Thomas's instructions: \"Extract relevant data from h5 file (h_canopy, h_canopy_uncertainty, night_flag, lat, long, and file name (because the beam and date is contained in the name) into text or whatever (we already have some code to do this).   You will want a file with the following information from icesat: Shotnumber, date, beam (or ground track), lat, lon, h_canopy and associated metrics, 20m segment heights, night_flag, h_canopy_uncertainty.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Also want to grab any parameter related to satellite motion (ascension/descension). Thinking  'beam_number_here/orbit_info/sc_orient' is the relevant metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Iterating through the beams array to make sure they're read in correctly. Extracting relevant parameters from within the land_segments group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High level overview of code: \n",
    "# For every file in our 5000 file dataset, and for every beam of the 6 beams within that specific file, we'll assign variables to parameters of interest (lat, long, night_flag) and write those values to csv \n",
    "\n",
    "# Outer loop: current file in subdirectory\n",
    "# Inner loop: beams of current file\n",
    "# Innermost code: assigning, reshaping, and getting all parameters into a single table that we'll write to csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-21-23:19:25\n"
     ]
    }
   ],
   "source": [
    "date_time = datetime.strptime(input_file[input_file.index('_') + 1:input_file.index('_') + 15], \"%Y%m%d%H%M%S\")\n",
    "print(date_time.strftime('%Y-%m-%d-%H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file: ATL08_20230620104617_00042002_006_01.h5\n",
      "beam name: gt1l\n",
      "beam name: gt1r\n",
      "beam name: gt2l\n",
      "beam name: gt2r\n",
      "beam name: gt3l\n",
      "beam name: gt3r\n",
      "Current file: ATL08_20230621231925_00272006_006_01.h5\n",
      "beam name: gt1l\n",
      "beam name: gt1r\n",
      "beam name: gt2l\n",
      "beam name: gt2r\n",
      "beam name: gt3l\n",
      "beam name: gt3r\n"
     ]
    }
   ],
   "source": [
    "# Creating an output file and clearing it's contents\n",
    "output_file = \"./Extracted Data/test_output.txt\"\n",
    "open(output_file, 'w').close()\n",
    "\n",
    "\n",
    "for filename in os.listdir(raw_data):\n",
    "    file = os.path.join(raw_data, filename)\n",
    "    \n",
    "    # checking if it is a file, assigning an array to all the file's keys\n",
    "    if os.path.isfile(file):\n",
    "        file = h5py.File(file, 'r')\n",
    "        keys = np.array(list(file.keys()))\n",
    "        beams = keys[5:11]\n",
    "        \n",
    "        print('Current file:', filename)\n",
    "        for beam in beams:\n",
    "            \n",
    "            \n",
    "            \n",
    "            latitude = file[beam + '/land_segments/latitude']\n",
    "            latitude = np.array((latitude)).reshape((latitude.shape[0], 1))\n",
    "            \n",
    "            longitude = file[beam + '/land_segments/latitude']\n",
    "            longitude = np.array((longitude)).reshape((longitude.shape[0], 1))\n",
    "            \n",
    "            # Column for canopy height             \n",
    "            h_canopy = file[beam + '/land_segments/canopy/h_canopy']\n",
    "            h_canopy = np.array(h_canopy).reshape((h_canopy.shape[0], 1))       \n",
    "            \n",
    "            # Column for canopy height uncertainty\n",
    "            h_canopy_uncertainty = file[beam + '/land_segments/canopy/h_canopy_uncertainty']\n",
    "            h_canopy_uncertainty = np.array(h_canopy_uncertainty).reshape((h_canopy_uncertainty.shape[0], 1))        \n",
    "            \n",
    "            \n",
    "            # This metric is NOT reshaped into a 1 column array, it has dimensions of ( number of transects x 18)\n",
    "            canopy_h_metrics = file[beam + '/land_segments/canopy/canopy_h_metrics']\n",
    "            canopy_h_metrics = np.array(canopy_h_metrics)\n",
    "            \n",
    "            # This metric is also NOT reshaped into a 1 column array, it has dimensions of ( number of transects x 5)\n",
    "            h_canopy_20m = file[beam + '/land_segments/canopy/h_canopy_20m/']\n",
    "            h_canopy_20m = np.array(h_canopy_20m)\n",
    "            \n",
    "            # Column for the canopy openness\n",
    "            canopy_openness = file[beam + '/land_segments/canopy/canopy_openness']\n",
    "            canopy_openness = np.array(canopy_openness).reshape((canopy_openness.shape[0], 1))\n",
    "            \n",
    "            ## Double check this, dimensions are wildly different than other metrics\n",
    "            # Column for photon segment ID\n",
    "            # segment_id = file[beam + '/signal_photons/ph_segment_id']\n",
    "            # segment_id = np.array(segment_id)\n",
    "            # print(segment_id.shape)\n",
    "            \n",
    "            # Column for the segment ID beginning\n",
    "            segment_id_beg = file[beam + '/land_segments/segment_id_beg']\n",
    "            segment_id_beg = np.array(segment_id_beg).reshape((segment_id_beg.shape[0], 1))\n",
    "            \n",
    "            # Column for the segment ID ending\n",
    "            segment_id_end = file[beam + '/land_segments/segment_id_end']\n",
    "            segment_id_end = np.array(segment_id_end).reshape((segment_id_end.shape[0], 1))\n",
    "            \n",
    "            # Column for the night flag\n",
    "            night_flag = file[beam + '/land_segments/night_flag']\n",
    "            night_flag = np.array(night_flag).reshape((night_flag.shape[0], 1))\n",
    "            \n",
    "            # Column for the specific file name, same dimensions as latitude column\n",
    "            file_name = np.full_like(latitude, filename, dtype=np.dtype('U100'))\n",
    "                     \n",
    "            # Creating a column for the granule date.\n",
    "            # HMS data can be parsed later as there's inconsistencies between file names and the granule START time listed in the Earthdata portal. \n",
    "            # Creating a datetime object of the filename string, then a column of just the datetime information\n",
    "            date_time = datetime.strptime(filename[filename.index('_') + 1:filename.index('_') + 15], \"%Y%m%d%H%M%S\")\n",
    "            date = np.full_like(latitude, date_time, dtype=np.dtype('U100'))\n",
    "            date = np.array(date).reshape((date.shape[0], 1))\n",
    "\n",
    "    \n",
    "            # Creating a column for the specific beam name, same dimensions as the latitude column\n",
    "            beam_name = np.full_like(latitude, beam, dtype=np.dtype('U100'))\n",
    "            # print('beam name:', beam)\n",
    "            \n",
    "            df = np.hstack((latitude, longitude, ))\n",
    "            \n",
    "            \n",
    "            with open(output_file, \"ab\") as f:\n",
    "                np.savetxt(f, df)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our loop through the 2 files, if append all the latitude information to a table we should have one with 136,000 rows\n",
    "lets test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200864,)\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File(input_file, 'r')\n",
    "# list(f['gt1r/signal'].keys())\n",
    "print(f['gt1r/signal_photons/ph_segment_id'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(1):\n",
    "    \n",
    "    \n",
    "      \n",
    "#     latitude = f[beams[i] + '/land_segments/latitude']\n",
    "#     latitude = np.array((latitude)).reshape((latitude.shape[0], 1))\n",
    "#     # print('latitude dimensions: ', latitude.shape)\n",
    "    \n",
    "#     longitude = f[beams[i] + '/land_segments/longitude']\n",
    "#     longitude = np.array((longitude)).reshape((longitude.shape[0], 1))\n",
    "#     # print('longitude dimensions: ', longitude.shape)\n",
    "    \n",
    "    \n",
    "#     h_canopy = f[beams[i] + '/land_segments/canopy/h_canopy']\n",
    "#     h_canopy = np.array(h_canopy).reshape((h_canopy.shape[0], 1))\n",
    "#     # print('h_canopy dimensions: ', h_canopy.shape)\n",
    "    \n",
    "    \n",
    "#     canopy_h_metrics = f[beams[i] + '/land_segments/canopy/canopy_h_metrics/']\n",
    "#     print('canopy_h_metrics dimensions: ', canopy_h_metrics.shape)\n",
    "    \n",
    "#     # Curious about this parameter, h_canopy_20m was transposed in Dr. Thomas's code\n",
    "#     h_canopy_20m = f[beams[i] + '/land_segments/canopy/h_canopy_20m/']\n",
    "#     # print('h_canopy_20m dimensions: ', h_canopy_20m.shape)/\n",
    "    \n",
    "    \n",
    "#     canopy_openness = f[beams[i] + '/land_segments/canopy/canopy_openness']\n",
    "#     canopy_openness = np.array(canopy_openness).reshape((canopy_openness.shape[0], 1))\n",
    "#     # print('canopy_openness dimensions: ', canopy_openness.shape)\n",
    "    \n",
    "#     segment_id_beg = f[beams[i] + '/land_segments/segment_id_beg']\n",
    "#     segment_id_beg = np.array(segment_id_beg).reshape((segment_id_beg.shape[0], 1))\n",
    "#     # print('segment_id_beg dimensions: ', segment_id_beg.shape)\n",
    "    \n",
    "#     segment_id_end = f[beams[i] + '/land_segments/segment_id_end']\n",
    "#     segment_id_end = np.array(segment_id_end).reshape((segment_id_end.shape[0], 1))\n",
    "#     # print('segment_id_end dimensions: ', segment_id_end.shape)\n",
    "    \n",
    "    \n",
    "#     night_flag = f[beams[i] + '/land_segments/night_flag']\n",
    "#     night_flag = np.array(night_flag).reshape((night_flag.shape[0], 1))\n",
    "#     # print('night_flag dimensions: ', night_flag.shape)\n",
    "    \n",
    "    \n",
    "#     # Creating column for just the beam name, will have the same dimensions as the latitude column\n",
    "#     beam_name = beams[i]\n",
    "#     beam = np.full_like(latitude, beam_name, dtype=np.dtype('U100'))\n",
    "   \n",
    "    \n",
    "#     df = np.hstack((beam, latitude, longitude, h_canopy, canopy_h_metrics, h_canopy_20m ))\n",
    "#     print(df[0:5, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
